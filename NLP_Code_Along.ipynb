{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepared By: Gaurav Nepal\n",
    "# NLP Code Along questions\n",
    "\n",
    "For this code along we will build a spam filter!\n",
    "\n",
    "We'll use a classic dataset for this - UCI Repository SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and  read the dataset,  have Spark infer the data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the pyspark functions that needed for this program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# using split from pyspark.sql.functions to split each 'sentence' in the DataFrame by its spaces:\n",
    "from pyspark.sql.functions import split \n",
    "#importing the Pyspark SQL length function to find the number of characters in each word:\n",
    "from pyspark.sql.functions import length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strart the Spark Session with app Name \"spamfilter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spamfilter').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read data  \"SMSSpamCollection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|ham\tGo until juro...|\n",
      "|ham\tOk lar... Jok...|\n",
      "|spam\tFree entry i...|\n",
      "|ham\tU dun say so ...|\n",
      "|ham\tNah I don't t...|\n",
      "|spam\tFreeMsg Hey ...|\n",
      "|ham\tEven my broth...|\n",
      "|ham\tAs per your r...|\n",
      "|spam\tWINNER!! As ...|\n",
      "|spam\tHad your mob...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text('SMSSpamCollection') #reading text file using spark.read,text\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check it's Schema \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting Data : \n",
    "As we can see above, there is only one Column as value and all the stored on single column. Lets split or convert the single column into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     split(value, \t)|\n",
      "+--------------------+\n",
      "|[ham, Go until ju...|\n",
      "|[ham, Ok lar... J...|\n",
      "|[spam, Free entry...|\n",
      "|[ham, U dun say s...|\n",
      "|[ham, Nah I don't...|\n",
      "|[spam, FreeMsg He...|\n",
      "|[ham, Even my bro...|\n",
      "|[ham, As per your...|\n",
      "|[spam, WINNER!! A...|\n",
      "|[spam, Had your m...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.select(split(df.value, \"\\t\")) #split each 'sentence' in the DataFrame by its tab, in order to make 2 columns later\n",
    "new_df.show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split or convert the single column into columns and name them class and text, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's create two columns 'class' and 'text' after spliting by tab '\\t', and store it in variable 'data' as it's mentioned below \n",
    "data = df.select(split(df.value, \"\\t\"))\\\n",
    "    .rdd.flatMap(\n",
    "            lambda x:x\n",
    "        ).toDF(schema=[\"class\", \"text\"]\n",
    "    )\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check it's Schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema is well set now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new length feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "length function to find the number of characters in each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#withColumn() funtion to create a new column names 'length'\n",
    "data = data.withColumn('length', length('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print the groupy mean of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.47192873420344|\n",
      "| spam|138.6760374832664|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy class and calculating it's mean value based on length\n",
    "data.groupBy('class').agg({'length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you transform you raw text in to tf_idf model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chain the transformer Tokenizer, StopWordsRemover, CountVectorizer and IDF for text to have a final column name 'tf_idf'\n",
    "- use the transformer StringIndexer for class column into output column 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create feature with vector assembler 'tf_idf','length of as input columns into output column named 'features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use pipeline for fit and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: it may differ for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StringIndexer,CountVectorizer, StopWordsRemover, IDF, VectorAssembler, HashingTF\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : https://spark.apache.org/docs/2.2.0/mllib-naive-bayes.html\n",
    "           : http://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "           : http://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_txt\")\n",
    "stop_remover = StopWordsRemover(inputCol='token_txt', outputCol='stop_token')\n",
    "count_vector = CountVectorizer(inputCol='stop_token', outputCol='count_vector') # fit a CountVectorizerModel\n",
    "idf = IDF(inputCol='count_vector', outputCol='output_idf')\n",
    "ham_spam_n = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = Pipeline(stages=[ham_spam_n, tokenizer, stop_remover, count_vector, idf, cleaning_data])\n",
    "cleaner = data_pipeline.fit(data)\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_data = VectorAssembler(inputCols=['output_idf', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13465,[7,11,31,6...|\n",
      "|  0.0|(13465,[0,24,296,...|\n",
      "|  1.0|(13465,[2,13,19,3...|\n",
      "|  0.0|(13465,[0,69,80,1...|\n",
      "|  0.0|(13465,[36,134,31...|\n",
      "|  1.0|(13465,[10,67,139...|\n",
      "|  0.0|(13465,[10,53,103...|\n",
      "|  0.0|(13465,[125,184,4...|\n",
      "|  1.0|(13465,[1,46,118,...|\n",
      "|  1.0|(13465,[0,1,13,27...|\n",
      "|  0.0|(13465,[18,43,120...|\n",
      "|  1.0|(13465,[8,17,37,8...|\n",
      "|  1.0|(13465,[13,30,46,...|\n",
      "|  0.0|(13465,[39,95,217...|\n",
      "|  0.0|(13465,[552,1690,...|\n",
      "|  1.0|(13465,[30,109,11...|\n",
      "|  0.0|(13465,[82,214,37...|\n",
      "|  0.0|(13465,[0,2,49,13...|\n",
      "|  0.0|(13465,[0,74,105,...|\n",
      "|  1.0|(13465,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Spam or Ham\n",
    "\n",
    "now use your tf-idf data to classify spam and ham\n",
    "\n",
    "feel free to use any classifier model\n",
    "\n",
    "result may differ for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = clean_data.randomSplit([0.7, 0.3])\n",
    "spam_detector = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = nb.fit(train) #NaiveBayes().fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13465,[0,1,2,7,8...|[-795.15568812323...|[1.0,1.9184131729...|       0.0|\n",
      "|  0.0|(13465,[0,1,2,13,...|[-604.84330271663...|[1.0,7.0434331863...|       0.0|\n",
      "|  0.0|(13465,[0,1,2,41,...|[-1061.9787139080...|[1.0,9.3946796811...|       0.0|\n",
      "|  0.0|(13465,[0,1,4,50,...|[-831.42583548862...|[1.0,8.8384745461...|       0.0|\n",
      "|  0.0|(13465,[0,1,5,20,...|[-809.69236939559...|[1.0,4.0528242366...|       0.0|\n",
      "|  0.0|(13465,[0,1,7,8,1...|[-1157.9825435245...|[1.0,4.4329686255...|       0.0|\n",
      "|  0.0|(13465,[0,1,12,33...|[-454.26028647698...|[1.0,6.2134460627...|       0.0|\n",
      "|  0.0|(13465,[0,1,14,78...|[-693.49068946716...|[1.0,1.4521604079...|       0.0|\n",
      "|  0.0|(13465,[0,1,43,68...|[-616.39946566409...|[0.99171300709555...|       0.0|\n",
      "|  0.0|(13465,[0,1,414,6...|[-297.24861100192...|[1.0,1.2601237369...|       0.0|\n",
      "|  0.0|(13465,[0,1,4614,...|[-144.18906576912...|[0.00436346472651...|       1.0|\n",
      "|  0.0|(13465,[0,2,3,4,6...|[-1279.5194623115...|[1.0,1.8310289325...|       0.0|\n",
      "|  0.0|(13465,[0,2,3,6,9...|[-3283.8326697398...|[1.0,1.0047812455...|       0.0|\n",
      "|  0.0|(13465,[0,2,4,8,1...|[-1334.7779175650...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(13465,[0,2,4,44,...|[-1913.9879996034...|[1.0,1.4552753259...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,8,3...|[-1137.9989164557...|[1.0,1.8248364949...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,11,...|[-729.00638831384...|[1.0,3.7028127215...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,31,...|[-656.65213780424...|[1.0,1.7070433460...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,43,...|[-590.42350257779...|[1.0,8.1924085381...|       0.0|\n",
      "|  0.0|(13465,[0,2,8,14,...|[-472.96195432676...|[1.0,9.3206860931...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = spam_detector.transform(test)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: \n",
      "0.9293625486190675\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "accuracy  = evaluator.evaluate(test_results)\n",
    "print(\"Test Accuracy: \")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9293625486190675 Which is not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
